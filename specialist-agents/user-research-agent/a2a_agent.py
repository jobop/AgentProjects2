#!/usr/bin/env python3
"""
çœŸæ­£çš„ç”¨æˆ·ç ”ç©¶Agent

å‚è€ƒcommon agentçš„MCPç®¡ç†æ–¹å¼ï¼Œä»mcp_servers.jsonè¯»å–çœŸå®é…ç½®
è®©LLMæ ¹æ®çœŸå®å¯ç”¨çš„MCPå·¥å…·å†³ç­–ä½¿ç”¨å“ªäº›å·¥å…·
"""

import asyncio
import json
import logging
import os
import sys
import httpx
from typing import Dict, Any, Optional, List
from python_a2a import A2AServer, skill, agent, run_server

# Add paths for imports
common_agent_src = os.path.join(os.path.dirname(__file__), "..", "..", "common-agent", "src")
shared_path = os.path.join(os.path.dirname(__file__), "..", "..")
sys.path.insert(0, common_agent_src)
sys.path.insert(0, shared_path)

try:
    from mcp.simple_mcp_loader import SimpleMCPLoader
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    class SimpleMCPLoader:
        def __init__(self):
            pass
        def load_config(self):
            return {}
        def list_available_servers(self):
            return []
        def get_server_config(self, name):
            return None

try:
    from shared.config_manager import get_timeout
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    def get_timeout(timeout_type):
        # é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆ10åˆ†é’Ÿï¼‰
        return 600

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@agent(
    name="ç”¨æˆ·ç ”ç©¶å‘˜-ViVi",
    description="çœŸæ­£çš„MCPé›†æˆç”¨æˆ·ç ”ç©¶ä¸“å®¶ï¼Œç”±LLMå†³ç­–ä½¿ç”¨MCPå·¥å…·è·å–çœŸå®æ•°æ®",
    version="1.0.0",
    url="http://localhost:8001",
    capabilities={
        "streaming": True,
        "google_a2a_compatible": True,
        "llm_powered": True,
        "real_mcp_integration": True,
        "no_simulation": True
    }
)
class RealMCPUserResearchAgent(A2AServer):
    """çœŸæ­£çš„MCPé›†æˆç”¨æˆ·ç ”ç©¶Agent - å‚è€ƒcommon agentæ–¹å¼"""
    
    def __init__(self):
        super().__init__()
        self.agent_id = "real_mcp_user_research_agent"
        self.agent_name = "Real MCP User Research Agent"
        
        # LLM APIé…ç½®
        self.api_key = os.getenv("SILICONFLOW_API_KEY", "xxxxx")
        self.base_url = "https://api.siliconflow.cn/v1"
        self.model = "deepseek-ai/DeepSeek-V3"
        
        # åˆå§‹åŒ–MCPç®¡ç†å™¨ - å‚è€ƒcommon agentæ–¹å¼
        self.mcp_loader = SimpleMCPLoader()
        self.available_mcp_servers = {}
        self.mcp_servers_info = {}
        
        self._init_mcp_system()
        
        logger.info("âœ… Real MCP User Research Agent åˆå§‹åŒ–å®Œæˆ")
    
    def _init_mcp_system(self):
        """åˆå§‹åŒ–MCPç³»ç»Ÿï¼Œå‚è€ƒcommon agentæ–¹å¼"""
        try:
            # åŠ è½½mcp_servers.jsoné…ç½®
            self.available_mcp_servers = self.mcp_loader.load_config()
            
            available_servers = self.mcp_loader.list_available_servers()
            logger.info(f"ğŸ”§ ä»mcp_servers.jsonåŠ è½½ {len(available_servers)} ä¸ªMCPæœåŠ¡å™¨")
            
            # è®°å½•æ‰€æœ‰å¯ç”¨çš„MCPæœåŠ¡å™¨ä¿¡æ¯ç»™LLMä½¿ç”¨
            for server_name in available_servers:
                server_config = self.mcp_loader.get_server_config(server_name)
                if server_config:
                    self.mcp_servers_info[server_name] = {
                        "name": server_name,
                        "description": server_config.get('description', 'No description'),
                        "command": server_config.get('command', 'unknown'),
                        "status": "available_for_llm_decision"
                    }
                    logger.info(f"  ğŸ“‹ {server_name}: {server_config.get('description', '')}")
            
            logger.info(f"âœ… MCPç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œ{len(self.mcp_servers_info)} ä¸ªæœåŠ¡å™¨å¯ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ MCPç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­è¿è¡Œï¼Œä½†æ²¡æœ‰MCPå·¥å…·
            self.mcp_servers_info = {}

    async def call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLM"""
        try:
            async with httpx.AsyncClient(timeout=get_timeout("llm_api")) as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": 2000,
                        "temperature": 0.7
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                    return f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                    
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return f"LLMè°ƒç”¨å¼‚å¸¸: {str(e)}"

    def _format_mcp_info_for_llm(self) -> str:
        """æ ¼å¼åŒ–MCPæœåŠ¡å™¨ä¿¡æ¯ç»™LLM"""
        if not self.mcp_servers_info:
            return "æš‚æ— å¯ç”¨çš„MCPæœåŠ¡å™¨"
        
        mcp_info = ["æˆ‘æœ‰ä»¥ä¸‹çœŸå®çš„MCPæœåŠ¡å™¨å¯ç”¨ï¼š"]
        for server_name, info in self.mcp_servers_info.items():
            mcp_info.append(f"- {server_name}: {info['description']}")
        
        mcp_info.append("\næ³¨æ„ï¼šä½ å¯ä»¥å†³ç­–ä½¿ç”¨è¿™äº›MCPæœåŠ¡å™¨ä¸­çš„ä»»ä½•å·¥å…·æ¥è·å–çœŸå®æ•°æ®ã€‚")
        return "\n".join(mcp_info)

    @skill(
        name="LLMé©±åŠ¨çš„å¸‚åœºç ”ç©¶",
        description="è®©LLMå†³ç­–ä½¿ç”¨å“ªäº›MCPå·¥å…·è¿›è¡ŒçœŸå®çš„å¸‚åœºç ”ç©¶ï¼Œä¸æ¨¡æ‹Ÿä»»ä½•æ•°æ®",
        tags=["llm-powered", "real-mcp", "market-research", "no-simulation"]
    )
    async def llm_driven_market_research(self, research_topic: str, research_depth: str = "comprehensive"):
        """LLMé©±åŠ¨çš„çœŸå®å¸‚åœºç ”ç©¶"""
        
        logger.info(f"ğŸ§  LLMé©±åŠ¨çš„å¸‚åœºç ”ç©¶: {research_topic}")
        
        # æ„å»ºåŒ…å«çœŸå®MCPæœåŠ¡å™¨ä¿¡æ¯çš„æç¤º
        mcp_info = self._format_mcp_info_for_llm()
        
        decision_prompt = f"""
ä½œä¸ºä¸“ä¸šçš„å¸‚åœºç ”ç©¶ä¸“å®¶ï¼Œæˆ‘éœ€è¦å¯¹"{research_topic}"è¿›è¡Œ{research_depth}çº§åˆ«çš„å¸‚åœºç ”ç©¶ã€‚

{mcp_info}

è¯·åˆ†æè¿™ä¸ªç ”ç©¶ä»»åŠ¡ï¼Œå†³å®šéœ€è¦ä½¿ç”¨å“ªäº›MCPæœåŠ¡å™¨å’Œå·¥å…·æ¥è·å–çœŸå®æ•°æ®ã€‚

è¿”å›JSONæ ¼å¼çš„æ‰§è¡Œè®¡åˆ’ï¼š
{{
    "task_analysis": "å¯¹ç ”ç©¶ä»»åŠ¡çš„åˆ†æ",
    "required_mcp_tools": [
        {{
            "server": "MCPæœåŠ¡å™¨åç§°",
            "purpose": "ä½¿ç”¨ç›®çš„",
            "data_type": "éœ€è¦è·å–çš„æ•°æ®ç±»å‹",
            "expected_outcome": "é¢„æœŸç»“æœ"
        }}
    ],
    "research_strategy": "ç ”ç©¶ç­–ç•¥è¯´æ˜",
    "methodology": "ç ”ç©¶æ–¹æ³•è®º"
}}

é‡è¦ï¼šåªä½¿ç”¨æˆ‘åˆ—å‡ºçš„çœŸå®MCPæœåŠ¡å™¨ï¼Œä½ å¯ä»¥æ¨æ–­æ¯ä¸ªæœåŠ¡å™¨å¯èƒ½æä¾›çš„å·¥å…·èƒ½åŠ›ã€‚
"""

        # LLMå†³ç­–ä½¿ç”¨å“ªäº›MCPå·¥å…·
        llm_response = await self.call_llm(decision_prompt)
        
        try:
            # è§£æLLMçš„å†³ç­–
            import re
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                decision = json.loads(json_match.group())
            else:
                decision = {"required_mcp_tools": [], "task_analysis": llm_response}
        except:
            decision = {"required_mcp_tools": [], "task_analysis": llm_response}
        
        # è®°å½•LLMçš„å·¥å…·å†³ç­–è®¡åˆ’
        tool_decision_plan = []
        required_tools = decision.get("required_mcp_tools", [])
        
        logger.info(f"ğŸ¯ LLMå†³ç­–ä½¿ç”¨ {len(required_tools)} ä¸ªMCPå·¥å…·")
        
        for tool_spec in required_tools:
            server = tool_spec.get("server")
            purpose = tool_spec.get("purpose", "æ•°æ®è·å–")
            
            # éªŒè¯LLMé€‰æ‹©çš„æœåŠ¡å™¨æ˜¯å¦çœŸå®å­˜åœ¨
            if server in self.mcp_servers_info:
                tool_decision_plan.append({
                    "server": server,
                    "purpose": purpose,
                    "data_type": tool_spec.get("data_type"),
                    "expected_outcome": tool_spec.get("expected_outcome"),
                    "server_info": self.mcp_servers_info[server],
                    "status": "llm_decided_real_server",
                    "note": "LLMé€‰æ‹©äº†çœŸå®å­˜åœ¨çš„MCPæœåŠ¡å™¨"
                })
                logger.info(f"âœ… LLMé€‰æ‹©çœŸå®æœåŠ¡å™¨: {server} - {purpose}")
            else:
                logger.warning(f"âš ï¸ LLMé€‰æ‹©äº†ä¸å­˜åœ¨çš„æœåŠ¡å™¨: {server}")
        
        # LLMåŸºäºå†³ç­–ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Šæ¡†æ¶
        if tool_decision_plan:
            synthesis_prompt = f"""
åŸºäºæˆ‘å¯¹MCPå·¥å…·çš„ä½¿ç”¨å†³ç­–ï¼Œä¸º"{research_topic}"ç”Ÿæˆä¸“ä¸šçš„å¸‚åœºç ”ç©¶æŠ¥å‘Šæ¡†æ¶ï¼š

æˆ‘çš„å·¥å…·ä½¿ç”¨å†³ç­–ï¼š
{json.dumps(tool_decision_plan, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„å¸‚åœºç ”ç©¶æŠ¥å‘Šæ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
1. ç ”ç©¶ç›®æ ‡å’ŒèŒƒå›´
2. æ•°æ®æ”¶é›†ç­–ç•¥ï¼ˆåŸºäºæˆ‘é€‰æ‹©çš„MCPå·¥å…·ï¼‰
3. åˆ†ææ–¹æ³•è®º
4. é¢„æœŸæ´å¯Ÿé¢†åŸŸ
5. æŠ¥å‘Šç»“æ„è®¾è®¡
6. æˆåŠŸæŒ‡æ ‡å®šä¹‰

åŸºäºä¸“ä¸šå¸‚åœºç ”ç©¶æ–¹æ³•è®ºå’ŒçœŸå®å·¥å…·èƒ½åŠ›ï¼Œæä¾›å¯æ‰§è¡Œçš„ç ”ç©¶æ¡†æ¶ã€‚
"""
            
            final_report_framework = await self.call_llm(synthesis_prompt)
        else:
            final_report_framework = "ç”±äºæ²¡æœ‰å¯ç”¨çš„MCPå·¥å…·ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿç ”ç©¶æ–¹æ³•è¿›è¡Œåˆ†æ"
        
        return {
            "success": True,
            "research_topic": research_topic,
            "research_depth": research_depth,
            "llm_decision": decision,
            "mcp_tool_decision_plan": tool_decision_plan,
            "final_research_framework": final_report_framework,
            "available_mcp_servers": len(self.mcp_servers_info),
            "llm_selected_tools": len(tool_decision_plan),
            "methodology": "LLMå†³ç­– + çœŸå®MCPæœåŠ¡å™¨ + ä¸“ä¸šç ”ç©¶æ–¹æ³•è®º",
            "message": f"âœ… LLMå®ŒæˆåŸºäºçœŸå®MCPé…ç½®çš„{research_topic}å¸‚åœºç ”ç©¶è§„åˆ’"
        }

    @skill(
        name="çœŸå®æ•°æ®ç”¨æˆ·ç”»åƒåˆ†æ",
        description="LLMå†³ç­–ä½¿ç”¨çœŸå®MCPå·¥å…·è·å–æ•°æ®ï¼Œè¿›è¡Œç”¨æˆ·ç”»åƒåˆ†æ",
        tags=["llm-powered", "real-data", "user-personas", "mcp-tools"]
    )
    async def llm_driven_persona_analysis(self, target_audience: str, analysis_goals: List[str]):
        """LLMå†³ç­–çš„ç”¨æˆ·ç”»åƒåˆ†æ"""
        
        logger.info(f"ğŸ§  LLMé©±åŠ¨çš„ç”¨æˆ·ç”»åƒåˆ†æ: {target_audience}")
        
        # LLMå†³ç­–æ•°æ®æ”¶é›†ç­–ç•¥
        mcp_info = self._format_mcp_info_for_llm()
        
        strategy_prompt = f"""
æˆ‘éœ€è¦ä¸º"{target_audience}"åˆ›å»ºç”¨æˆ·ç”»åƒï¼Œåˆ†æç›®æ ‡ï¼š
{json.dumps(analysis_goals, ensure_ascii=False, indent=2)}

{mcp_info}

è¯·åˆ¶å®šæ•°æ®æ”¶é›†å’Œåˆ†æç­–ç•¥ï¼Œå†³å®šä½¿ç”¨å“ªäº›MCPæœåŠ¡å™¨æ¥è·å–çœŸå®çš„ç”¨æˆ·æ•°æ®ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "analysis_approach": "åˆ†ææ–¹æ³•",
    "data_collection_plan": [
        {{
            "server": "MCPæœåŠ¡å™¨åç§°",
            "data_source": "æ•°æ®æ¥æº",
            "data_type": "æ•°æ®ç±»å‹",
            "purpose": "æ”¶é›†ç›®çš„",
            "expected_insight": "æœŸæœ›è·å¾—çš„æ´å¯Ÿ"
        }}
    ],
    "persona_framework": "ç”¨æˆ·ç”»åƒæ¡†æ¶è®¾è®¡"
}}
"""

        strategy_response = await self.call_llm(strategy_prompt)
        
        try:
            import re
            json_match = re.search(r'\{.*\}', strategy_response, re.DOTALL)
            if json_match:
                strategy = json.loads(json_match.group())
            else:
                strategy = {"data_collection_plan": []}
        except:
            strategy = {"data_collection_plan": []}
        
        # éªŒè¯LLMçš„æ•°æ®æ”¶é›†è®¡åˆ’
        validated_plan = []
        data_plan = strategy.get("data_collection_plan", [])
        
        for data_spec in data_plan:
            server = data_spec.get("server")
            if server in self.mcp_servers_info:
                validated_plan.append({
                    **data_spec,
                    "server_info": self.mcp_servers_info[server],
                    "validation_status": "real_server_selected"
                })
                logger.info(f"âœ… éªŒè¯é€šè¿‡: {server} - {data_spec.get('purpose')}")
        
        # LLMç”Ÿæˆç”¨æˆ·ç”»åƒåˆ†ææ¡†æ¶
        if validated_plan:
            persona_prompt = f"""
åŸºäºéªŒè¯çš„æ•°æ®æ”¶é›†è®¡åˆ’ï¼Œä¸º"{target_audience}"è®¾è®¡ç”¨æˆ·ç”»åƒåˆ†ææ¡†æ¶ï¼š

éªŒè¯çš„æ•°æ®æ”¶é›†è®¡åˆ’ï¼š
{json.dumps(validated_plan, ensure_ascii=False, indent=2)}

åˆ†æç›®æ ‡ï¼š
{json.dumps(analysis_goals, ensure_ascii=False, indent=2)}

è¯·è®¾è®¡è¯¦ç»†çš„ç”¨æˆ·ç”»åƒåˆ†ææ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
1. ç”¨æˆ·ç”»åƒç»´åº¦è®¾è®¡
2. æ•°æ®åˆ†ææ–¹æ³•
3. æ´å¯Ÿæå–ç­–ç•¥
4. ç”»åƒéªŒè¯æ–¹æ³•
5. åº”ç”¨åœºæ™¯è®¾è®¡

åŸºäºçœŸå®æ•°æ®æºå’Œä¸“ä¸šç”¨æˆ·ç ”ç©¶æ–¹æ³•è®ºã€‚
"""
            
            personas_framework = await self.call_llm(persona_prompt)
        else:
            personas_framework = "ç”±äºæ²¡æœ‰åˆé€‚çš„MCPæ•°æ®æºï¼Œå°†ä½¿ç”¨å®šæ€§ç ”ç©¶æ–¹æ³•"
        
        return {
            "success": True,
            "target_audience": target_audience,
            "analysis_goals": analysis_goals,
            "data_collection_strategy": strategy,
            "validated_data_plan": validated_plan,
            "user_personas_framework": personas_framework,
            "methodology": "LLMå†³ç­–æ•°æ®æ”¶é›† + çœŸå®MCPæœåŠ¡å™¨ + ä¸“ä¸šç”»åƒåˆ†æ",
            "message": f"âœ… å®ŒæˆåŸºäºçœŸå®MCPé…ç½®çš„{target_audience}ç”¨æˆ·ç”»åƒåˆ†æè§„åˆ’"
        }

    # @skill(
    #     name="MCPæœåŠ¡å™¨çŠ¶æ€æ£€æŸ¥",
    #     description="æ£€æŸ¥å½“å‰å¯ç”¨çš„MCPæœåŠ¡å™¨çŠ¶æ€",
    #     tags=["mcp-status", "system-check", "real-time"]
    # )
    # async def check_mcp_servers_status(self):
    #     """æ£€æŸ¥MCPæœåŠ¡å™¨çŠ¶æ€"""
        
    #     return {
    #         "success": True,
    #         "mcp_system_status": "loaded_from_config",
    #         "total_servers": len(self.mcp_servers_info),
    #         "server_details": self.mcp_servers_info,
    #         "config_source": "mcp_servers.json",
    #         "integration_method": "SimpleMCPLoaderæ–¹å¼",
    #         "message": f"âœ… MCPçŠ¶æ€: {len(self.mcp_servers_info)}ä¸ªæœåŠ¡å™¨ä»é…ç½®æ–‡ä»¶åŠ è½½"
    #     }


def main():
    """ä¸»å‡½æ•°"""
    port = int(os.environ.get("AGENT_PORT", 8001))
    
    agent_instance = RealMCPUserResearchAgent()
    logger.info(f"ğŸš€ å¯åŠ¨Real MCP User Research Agentï¼Œç«¯å£: {port}")
    logger.info("ğŸ§  æ­¤Agentå‚è€ƒcommon agentæ–¹å¼ï¼Œä»mcp_servers.jsonè¯»å–çœŸå®é…ç½®")
    logger.info("ğŸ¯ LLMå†³ç­–ä½¿ç”¨çœŸå®MCPå·¥å…·ï¼Œæ— ä»»ä½•ç¡¬ç¼–ç æˆ–æ¨¡æ‹Ÿæ•°æ®")
    run_server(agent_instance, host="0.0.0.0", port=port)


if __name__ == "__main__":
    main() 
